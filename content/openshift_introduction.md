Здравствуйте уважаемые участники ИТ сообщества. Меня зовут Олег, я работаю в компании, которая занимается разработкой ПО. Я занимаюсь ручным и автоматизированным тестированием Linux и Unix продуктов и я хотел бы поделиться положительным опытом автоматизированного тестирования в [Openshift Origin](https://www.openshift.org/).

**Цели, которые я преследую**:

1. Донести до русскоязычного сообщества особенности работы с Openshift Origin в контексте тестирования.
2. Рассказать о преимуществах и недостатках тестирования в контейнерах.
3. Агрегировать и актуализировать свои знания о [Kubernetes](https://kubernetes.io/)/Openshift.


**Весь материал изложен в трёх статьях**:

1. [Тестирование в Openshift: Введение](https://habrahabr.ru/post/332994/)
2. [Тестирование в Openshift: Внутреннее устройство кластера](https://habrahabr.ru/post/333012/)
3. [Тестирование в Openshift: Автоматизированное тестирование](https://habrahabr.ru/post/333014/)

*Примечание: хотелось бы сразу заметить, что излагаемый материал касается Openshift v3, а не Openshift v2 (когда компания Red Hat еще не начала использовать Kubernetes в качестве ядра для своих продуктов и сервисов).*

<cut/>

**Почему был выбран Openshift Origin**:

Так случилось, что после трудоустройства на текущее место работы, выяснилось, что автоматизированного тестирования нет в принципе. Проработав несколько месяцев в ручном режиме пришло понимание о необходимости двух вещей: инструмента для развертывания и поддержки тестируемых сред, framework для написания тестов.

На первоначальном этапе для развертывания и поддержки сред был организован автоматически обновляемый репозиторий [Vagrant](https://en.wikipedia.org/wiki/Vagrant_(software)) ([VirtualBox](https://www.virtualbox.org/)), который обновляет и упаковывает различные ОС в автоматическом режиме. Это стало подспорьем не только для тестирования, но и для разработки, потому как: виртуальные машины были сконфигурированы согласно выработанным требованиям; виртуальные машины были обновлены, были предустановлены необходимые инструменты; имелись сценарии Vagrant для развертывания сложных окружений. Стоит отметить, что все среды Vagrant разворачиваются на локальных машинах участников разработки, а не в выделенном [IaaS](https://en.wikipedia.org/wiki/Cloud_computing#Infrastructure_as_a_service_.28IaaS.29), что ожидаемо вызывало проблемы производительности рабочих станций.

Было выиграно время, стало удобнее работать, появились некий стандарт и предсказуемость, но главная проблема оставалась - долгое развертывание тестируемых сред (полная виртуализация, различные дистрибутивы Linux, дополнительные сервисы (которые участвуют в тестировании)). Развертывание тестируемых сред могло занимать десятки минут, в то время как сам тест проходил за считанные секунды. При этом количество автоматизированных тестов росло и ожидать результатов приходилось всё дольше.

Закономерным стал вопрос об организации обособленного IaaS, в котором происходили бы все задачи тестирования, но данный подход был не идеален: всё так же использовалась бы полная виртуализация, для построения IaaS требовались бы финансовые вложения на покупку аппаратного обеспечения (небольшой парк слабых рабочих станции присутствовал).   

Следующим этапом стала проработка быстрого (в контексте быстродействия окружений) [CI](https://en.wikipedia.org/wiki/Continuous_integration)/[CD](https://en.wikipedia.org/wiki/Continuous_delivery) c учетом новых требовании, а именно (основные, согласно приоритетам):

1. Open Source решение без каких-либо затрат на покупку ПО.
2. Быстрая скорость развертывания тестируемых сред. Связность и осведомленность о существовании сервисов между средами. Встроенная поддержка и контроль запусков тестов внутри окружения. Высокая плотность размещения для удовлетворения пункта 4.
3. Активное сообщество, популярный проект с прогнозируемым будущем, репутация владельца, наличие документации.
4. Отсутствие дополнительных вложений в аппаратную составляющую.

Я не буду утруждать читателей подробным пересказом о тернистом пути поиска подходящих продуктов, но хочу ознакомить со списком рассмотренного ПО (февраль 2016): 

1. Замена полной виртуализации: [Docker](https://www.docker.com/), [LXC](https://linuxcontainers.org), [OpenVZ](https://openvz.org), [kvmtool](https://github.com/penberg/linux-kvm/tree/master/tools/kvm).
2. Оркестрация и управление кластером: [Kubernetes](https://kubernetes.io/), [Nomad](https://www.nomadproject.io/), [Openshift](https://www.openshift.org/), [Openstack](https://www.openstack.org/), [Swarm](https://docs.docker.com/engine/swarm/).
3. Поддержка в [Jenkins](https://jenkins.io/): [Docker plugin](https://plugins.jenkins.io/docker-plugin), [Kubernetes plugin](https://plugins.jenkins.io/kubernetes), [Openshift plugin](https://github.com/openshift/jenkins-plugin)

Одни инструменты плохо управлялись или требовали слишком много поддержки и обслуживания, другие плохо интегрировались с Jenkins и не предоставляли нужного функционала, третьи только начинали свой путь. Однозначным лидером вырисовывалась Kubernetes и её производные (коих не мало), но в чистом виде платформа от Google была сложна в освоении и развертывании (разработчики прилагают большие усилия для упрощения использования данной платформы). 

**Своими словами oб Openshift Origin:**

Openshift Origin - это в первую очередь Open Source платформа для разработки и публикации ПО, которая базируется на платформе оркестрации контейнеров Kubernetes.  Платформа расширяет возможности Kubernetes с помощью специальных объектов и механизмов.

Кластер:

Кластер может быть развернут или обновлен с помощью сценариев [Ansible](https://github.com/openshift/openshift-ansible). Внутри кластера может использоваться как встроенный так и внешний Docker регистр. Узлы кластера могут быть закреплены на основе специальных меток за отдельными проектами. Присутствует [сборщик мусора](https://docs.openshift.org/latest/admin_guide/garbage_collection.html) и [настраиваемый планировщик](https://docs.openshift.org/latest/admin_guide/scheduler.html). Кластер может быть развернут внутри Openstack и интегрирован с ним с целью автоматического масштабирования и предоставления хранилища данных. Среды могут быть осведомлены об опубликованных сервисах и других узлах с помощью переменных окружений и DNS имен. Возможна проверка доступности сервисов через HTTP/TCP запрос и/или через выполнение команды внутри контейнера. Ресурсы кластера могут быть [квотированы](https://docs.openshift.org/latest/dev_guide/compute_resources.html) на уровне проектов (процессор, память, количество объектов и т.д.). Присутствует возможность мониторинга кластера на уровне контейнеров и рабочих узлов.

Данные:

В качестве способа харанения данных могут выступать временные или постоянные тома, которые монтируются непосредственно в контейнеры. Backend для данных томов могут выступать: [NFS, GlusterFS, OpenStack Cinder, Ceph RBD, AWS Elastic Block Store (EBS), GCE Persistent Disk и т.д.](https://docs.openshift.org/latest/install_config/persistent_storage/index.html) Присутствует возможность (hostPath) монтирования локального каталога того рабочего узла, на котором запущен контейнер.

Сеть:

По умолчанию все коммуникации осуществляются с помощью [Open vSwitch](http://openvswitch.org/), но существует поддержка других сетевых решений через систему плагинов Kubernetes. По умолчанию все среды могу коммуницировать друг с другом, но возможна изоляция на уровне проектов. Поддерживается разрешение DNS имен сервисов с помощью встроенной службы [SkyDNS](https://github.com/skynetservices/skydns). Опубликованные внутри кластера сервисы могут быть доступны извне. Всю функциональность по изоляции и функциям NAT берет на себя [Iptables](https://en.wikipedia.org/wiki/Iptables). 

Безопасность:

Разграничение прав пользователей на основе ролей. Изоляция контейнеров с помощью [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) (и не только). Поддержка секретов, которые используются для доступа к различным ресурсам. Все коммуникации между рабочими узлами кластера и мастером осуществляется через шифрованные соединения (создается CA, выдаются клиентские сертификаты). 

Управление:

Доступны интерфейсы API как для самого Openshift, так и для Kubernetes. Доступен кроссплатформенный [консольный клиент](https://docs.openshift.org/latest/cli_reference/get_started_cli.html). Доступен веб-интерфейс, с помощью которого пользователи могут: работать в изолированных проектах с учетом их полномочий, взаимодействовать с запущенными в кластере контейнерами, обозревать созданные объекты, отслеживать события и т.д.

**Заключение:**

Несмотря на обилие технологий и продуктов, которые представлены на рынке, найти подходящий инструмент достаточно сложно. Балансируя между унификацией и интеграцией процессов CI/CD, учитывая сложность сопровождения, отслеживая цепочки задействованных технологий, всё же была найдена Kubernetes, а затем и Openshift Origin. В отличии от базовой платформы оркестрации контейнерами Kubernetes, Openshift привносит необходимые элементы для удобной и эффективной работы.